# AI's Gift Recommendation

Youtube Link: https://www.youtube.com/watch?v=cUgiD5I4dtM

### Documentation

#### Overview
This project implements a chatbot that interacts with users to understand their needs and preferences, providing recommendations based on a predefined mock dataset. The chatbot uses Retrieval-Augmented Generation (RAG) and a vector database to store and retrieve information.

#### Components

1. **Environment Setup:**
   - Load environment variables for API keys.
   - Configure the Google Generative AI.

2. **Mock Data:**
   - Manually created mock data that includes fields such as age, gender, relationship, occasion, budget, max budget, gift, rating, link, image link, and interest.

3. **Text Splitting:**
   - The `get_text_chunks` function splits the mock data into chunks for better indexing and retrieval using a text splitter.

4. **Vector Store:**
   - The `get_vector_store` function stores text chunks in a FAISS vector store using embeddings generated by Google Generative AI.

5. **Conversational Chain:**
   - The `get_conversational_chain` function defines the prompt template and creates a question-answering chain using LangChain and Google Generative AI.

6. **User Interaction:**
   - The `user_input` function processes user questions, retrieves relevant documents from the vector store, and generates responses using the conversational chain.

7. **Streamlit Interface:**
   - The `main` function sets up the Streamlit interface, allowing users to input questions and generate/store mock data.

#### Detailed Steps

1. **Load Environment Variables:**
   - Ensure that the environment variables, such as the Google API key, are loaded using `dotenv`.

2. **Configure Google Generative AI:**
   - Configure the generative AI with the loaded API key.

3. **Define Mock Data:**
   - Manually define a list of mock data entries, each representing a different scenario based on the dataset.

4. **Split Text into Chunks:**
   - Use the `RecursiveCharacterTextSplitter` to split each mock data entry into manageable chunks, ensuring efficient retrieval.

5. **Store Text Chunks:**
   - Convert the text chunks into embeddings and store them in a FAISS vector store for fast similarity search.

6. **Define Conversational Chain:**
   - Create a conversational chain using a prompt template that specifies how to use the context to answer questions accurately.

7. **Process User Questions:**
   - When a user inputs a question, search the vector store for the top 3 relevant documents and generate a response using the conversational chain.

8. **Streamlit Interface:**
   - Set up the Streamlit interface to allow users to input questions and generate/store mock data. Display the chatbotâ€™s responses in the Streamlit app.

By following this implementation, the chatbot can effectively process user queries and provide relevant, sensible, and accurate responses based on the mock dataset. This setup aligns with the requirements of using a predefined dataset and generating recommendations based on user input using RAG.
